Pre-trained models used in our paper

Most results are from E16-S8-1 (with a 16D encoding and 8 stores).

E16-S8-2 has the same parameters and was retrained with a different random initialization.
